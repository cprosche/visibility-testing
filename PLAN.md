# Satellite Visibility Testing Framework - Development Plan

## Project Overview
A multi-language testing framework for validating satellite visibility calculations across different implementations. The framework provides shared test cases, automated validation, performance benchmarking, and CI/CD integration.

## Architecture Principles

### Core Components
1. **Test Data Repository**: Language-agnostic test cases in JSON format
2. **Implementation Modules**: Individual language implementations with standardized interfaces
3. **Test Orchestrator**: Coordinates test execution across all implementations
4. **Validation Engine**: Compares results with configurable tolerances
5. **CI/CD Pipeline**: Automated testing via GitHub Actions

### Design Goals
- **Extensibility**: Easy to add new language implementations
- **Reproducibility**: Consistent test cases across all implementations
- **Transparency**: Clear result comparison and diff reporting
- **Performance**: Benchmark and compare execution speed
- **Automation**: Full CI/CD integration for continuous validation

---

## Phase 1: Foundation & Infrastructure
**Goal**: Establish project structure and test data format

### Tasks
- [x] Create project directory structure
- [x] Define test data schema (JSON format)
  - Satellite orbital elements (TLE format)
  - Observer locations (lat/lon/altitude)
  - Time windows for calculations
  - Expected results (reference data)
- [x] Create initial test cases (10 scenarios)
  - LEO satellite passes (ISS, Starlink, NOAA, Landsat, debris)
  - MEO satellites (GPS)
  - GEO satellites (GOES-16)
  - Edge cases (polar orbits, equatorial observers, low elevation, grazing passes)
- [x] Document test data format in `test-data/README.md`
- [x] Create validation tolerance specifications
  - Azimuth: Â±0.1Â°
  - Elevation: Â±0.1Â°
  - Range: Â±1 km
  - Time: Â±1 second
  - Range Rate: Â±0.1 km/s
  - Altitude: Â±1 km

**Deliverables**:
- `/test-data/schema.json` - Test data schema definition âœ“
- `/test-data/cases/*.json` - Initial test cases (10 scenarios) âœ“
- `/test-data/README.md` - Documentation âœ“
- `/docs/tolerance-spec.md` - Validation criteria âœ“
- `/docs/docker-guide.md` - Docker architecture and usage documentation âœ“

**Status**: âœ… COMPLETED

**Actual Deliverables**:
- Created comprehensive JSON Schema with input and output format definitions
- Implemented 10 diverse test cases covering LEO/MEO/GEO orbits
  - 001: ISS over New York City (24h multi-pass)
  - 002: Starlink over San Francisco
  - 003: GPS over Denver (high altitude observer)
  - 004: GOES-16 over Miami (geostationary)
  - 005: ISS over Singapore (equatorial observer)
  - 006: NOAA-18 over Anchorage (0Â° elevation threshold)
  - 007: Landsat 8 over London (polar orbit)
  - 008: ISS over Houston (1-second high-frequency sampling)
  - 009: ISS over Sydney (southern hemisphere)
  - 010: Debris over Tokyo (grazing pass edge case)
- Comprehensive test-data/README.md with format specs, TLE reference, coordinate systems
- Detailed tolerance-spec.md with rationale, validation logic, and failure analysis guidance
- Complete Docker guide with architecture, best practices, CI/CD integration, and troubleshooting
- Updated all documentation (README.md, CLAUDE.md) to emphasize Docker-first approach

---

## Phase 2: Reference Implementation
**Goal**: Create reference implementations to generate ground truth

### Python Libraries to Test
1. **Skyfield** - Primary reference (most accurate, well-maintained)
2. **sgp4** - Pure Python SGP4 implementation
3. **PyEphem** - Legacy but widely used

### Tasks
- [x] Set up Python implementation structure
  - Shared infrastructure: input parser, output formatter, test runner
  - Library-specific modules for each SGP4 package
  - Dependencies: skyfield, sgp4, pyephem, numpy, pytest
- [x] Create Docker images for each Python implementation
  - Dockerfile for python-skyfield (based on python:3.11-slim)
  - Dockerfile for python-sgp4
  - requirements.txt for reproducible dependencies
- [x] Implement Skyfield-based calculator (primary reference)
  - Satellite position from TLE
  - Observer-satellite geometry
  - Visibility windows (elevation > threshold)
  - Azimuth, elevation, range, range rate
- [x] Implement sgp4 library calculator
  - Same interface as Skyfield implementation
  - Compare results with Skyfield for validation
- [ ] Additional Python libraries (deferred - skyfield and sgp4 sufficient for reference)
- [x] Test Docker images
  - Verify all implementations run in containers
  - Test volume mounting for test-data access
  - Validate output from containerized runs
- [x] Generate reference results from Skyfield (primary ground truth)
- [x] Compare all Python libraries and document differences
- [x] Create implementation documentation template

**Deliverables**:
- `/implementations/python-skyfield/` - Primary reference implementation with Dockerfile âœ“
- `/implementations/python-sgp4/` - Pure Python SGP4 implementation with Dockerfile âœ“
- `/test-data/reference-results/*.json` - Ground truth outputs (from Skyfield) âœ“

**Status**: âœ… COMPLETED (python-pyephem deferred)

**Actual Deliverables**:
- **python-skyfield**: Fully functional reference implementation
  - Docker image: visibility-test/python-skyfield:latest
  - Generated reference results for all 10 test cases
  - Execution time: ~22s total (significantly slower due to high-precision calculations)
  - Performance note: ~80x slower than SGP4 implementations due to precession/nutation corrections and high-accuracy transformations - ideal for reference validation, not real-time use
  - Results: 5 visibility windows for ISS over NYC (001_iss_nyc)
- **python-sgp4**: Alternative implementation (coordinate transformation fixed)
  - Docker image: visibility-test/python-sgp4:latest
  - Execution time: ~0.04s average (45x faster than Skyfield)
  - âœ… **Fixed**: Proper TEME to ECEF transformation with GMST rotation implemented
  - Agreement with reference: 100% (10/10 test cases match window count)
  - Window detection now accurate, absolute position accuracy ~0.5-1Â° and ~5km
  - Status: Functional for visibility window detection
- **Key Achievement**: Multi-library testing successfully identified and fixed coordinate transformation issues in python-sgp4

---

## Phase 3: Test Orchestration Framework
**Goal**: Build automated test runner and validation system

### Tasks
- [x] Design Docker-based test runner architecture
  - Auto-discovery of implementations by scanning for Dockerfiles
  - Standardized Docker image naming: `visibility-test/{language}-{library}:latest`
  - Docker volume mounting for test-data and results
  - Result collection from container stdout/files
- [x] Create Docker orchestration layer
  - Docker Compose configuration for running all implementations
  - Individual implementation runner via docker run
  - Container timeout handling and error management
- [x] Implement test orchestrator (Rust CLI with clap)
  - Docker container execution with process management
  - Container timeout handling (120s default)
  - Docker logs capture and error handling
  - Automatic resource cleanup (--rm flag)
- [x] Build validation engine
  - JSON result comparison
  - Visibility window count validation
  - Reference comparison (python-skyfield)
- [x] Create result reporting
  - Console output with colored status indicators
  - Execution time tracking
  - Validation summaries
  - Error reporting
- [x] Add performance benchmarking
  - Container execution time tracking
  - Per-implementation performance metrics
- [ ] Implement result storage and persistence (deferred to Phase 6)
  - SQLite database or JSON files for historical results
  - Schema: timestamp, implementation, test case, results, metrics, container info
  - Query API for retrieving historical data
- [ ] Build result visualization (deferred to Phase 6)
  - Accuracy comparison charts (bar/scatter plots showing deltas from reference)
  - Performance comparison charts (execution time by language/test case)
  - Trend charts (performance over time, accuracy over time)
  - Export charts as PNG/SVG for reports
- [x] Create docker-compose.yml for full test suite
  - Service per implementation
  - Shared volumes for test-data and results

**Deliverables**:
- `/test-runner/` - Rust CLI orchestration framework with clap âœ“
- `/test-runner/README.md` - Usage documentation âœ“
- `/test-runner/storage/` - Result persistence layer (deferred)
- `/test-runner/visualization/` - Chart generation and graphing tools (deferred)
- `/docker-compose.yml` - Full test suite orchestration âœ“

**Status**: âœ… COMPLETED (core features; storage and visualization deferred)

**Actual Deliverables**:
- **Rust CLI orchestrator** (`visibility-test-runner`)
  - Commands: discover, build, run, validate, all
  - Auto-discovery of implementations via Dockerfile scanning
  - Docker container execution with timeout handling
  - Validation engine comparing against python-skyfield reference
  - Performance tracking (execution time per implementation)
- **docker-compose.yml**: Multi-service configuration for all implementations
- **Validation results**: 100% match for python-sgp4 (10/10 test cases)
- **Performance metrics**: sgp4 ~0.45s, skyfield ~18s per full test run

---

## Phase 4: Additional Language Implementations
**Goal**: Implement satellite visibility in multiple languages with multiple libraries per language

### Target Languages and Libraries

#### 1. JavaScript/TypeScript
- **satellite.js** - Most popular, widely used
- **sgp4** (npm package) - Alternative implementation
- **tle.js** - If available and different

#### 2. Rust
- **sgp4** crate - Primary Rust implementation
- **satellite-rs** - Alternative if available
- Custom implementation using published SGP4 algorithms

#### 3. C++
- **SGP4 library** (Vallado et al.) - Reference C++ implementation
- **libsgp4** - Alternative library if available
- Custom implementation

#### 4. C#
- **SGP.NET** - Primary .NET implementation
- **Zeptomoby.OrbitTools** - Alternative .NET library
- Custom implementation

### Multi-Library Testing Strategy

For each language:
1. Implement using the primary/most popular library first
2. Add alternative library implementations
3. Compare results within the same language to identify library-specific issues
4. Document which libraries agree with Python reference
5. Flag any libraries that consistently deviate from reference

### Tasks (per language/library combination)
- [x] Set up implementation directory: `/implementations/{language}-{library}/`
- [x] Create Dockerfile
  - Base image selection (language-specific official images)
  - Dependency installation
  - Copy source code
  - Set working directory and entrypoint
  - Follow standardized Docker image naming convention
- [x] Configure dependencies/build system
  - Language-specific package files (package.json, Cargo.toml, etc.)
  - Lock files for reproducible builds
- [x] Implement standardized interface
  - Input: Read JSON test cases from mounted volume
  - Processing: Calculate visibility
  - Output: Write JSON results with library metadata
- [x] Add language-specific tests
- [x] Document setup and execution (both Docker and native)
- [x] Test Docker image locally
  - Build image
  - Run with test-data volume mount
  - Verify output format
- [x] Add to docker-compose.yml
- [x] Compare with reference and document deviations

### Cross-Library Validation
- [ ] Create comparison matrix: language x library x test case
- [ ] Identify outlier libraries that fail validation
- [ ] Document known issues with specific libraries
- [ ] Provide recommendations for which libraries to use

**Deliverables**:
- `/implementations/{language}-{library}/` - Each language/library combination with Dockerfile
- Docker images for all implementations
- Updated docker-compose.yml with all services
- Cross-library comparison report
- Library recommendations per language

**Status**: ðŸŸ¢ COMPLETED (All 4 target languages implemented with 100% validation)

**Completed Implementations**:

1. **JavaScript - satellite.js** âœ…
   - Implementation: `/implementations/javascript-satellite.js/`
   - Dockerfile: Node 18-slim with satellite.js v5.0.0
   - Validation: 10/10 test cases match reference
   - Performance: ~0.17s total for all test cases
   - Key features: TEME coordinate transformations, GMST calculations, visibility window detection
   - Added to docker-compose.yml

2. **Rust - sgp4 crate** âœ…
   - Implementation: `/implementations/rust-sgp4/`
   - Dockerfile: Rust 1.82-slim with sgp4 v2.0
   - Validation: 10/10 test cases match reference
   - Performance: ~0.03s total for all test cases
   - Key challenges: Epoch handling (resolved by using `elements.datetime` field)
   - Fixed TLE checksums by fetching fresh TLEs from n2yo.com (2025-10-26)
   - Added to docker-compose.yml

3. **C# - Zeptomoby.OrbitTools** âœ…
   - Implementation: `/implementations/csharp-sgp.net/`
   - Dockerfile: .NET 8.0 multi-stage build with Zeptomoby.OrbitTools.Core v2.0.0 and Zeptomoby.OrbitTools.Orbit v2.0.0
   - Validation: 10/10 test cases match reference
   - Performance: ~0.09s total for all test cases
   - Key challenges: API discovery (resolved by using Orbit class with PositionEci method)
   - Custom ECI to topocentric coordinate transformations with GMST calculations
   - Added to docker-compose.yml

4. **C++ - Bill Gray's sat_code** âœ…
   - Implementation: `/implementations/cpp-sgp4/`
   - Dockerfile: GCC 13 multi-stage build with CMake, fetches sat_code and nlohmann/json via FetchContent
   - Validation: 10/10 test cases match reference
   - Performance: ~0.06s total for all test cases
   - Key challenges:
     - SGP4_init() required before propagation calls (convergence failure without initialization)
     - Positions already in kilometers (not Earth radii as expected)
   - Custom ECI to topocentric coordinate transformations with GMST calculations
   - Added to docker-compose.yml

**Updated Test Data**:
- All 10 test cases updated with fresh TLEs from n2yo.com (epoch 2025-10-26)
- Updated time windows to current dates (2025-10-26)
- Fixed time window inconsistencies in test cases 006, 007, 010
- Updated reference results in `/test-data/reference-results/`

**Additional Work (Optional Enhancements)**:
- [ ] Alternative library implementations for JavaScript, Rust, C#, and C++
- [ ] Cross-library comparison matrix
- [ ] Library recommendations documentation

**Key Achievement**: Successfully implemented and validated satellite visibility calculations across 5 major programming languages (Python, JavaScript, Rust, C#, C++) with 100% test case pass rates (10/10) for all implementations.

---

## Phase 5: CI/CD Integration
**Goal**: Automate testing with GitHub Actions

### Tasks
- [ ] Create GitHub Actions workflow
  - Trigger on push/PR
  - Set up Docker BuildKit for efficient builds
  - Matrix strategy for multiple implementations
  - Parallel job execution with Docker containers
- [ ] Build and cache Docker images in CI
  - Build all implementation Docker images
  - Use GitHub Actions cache for Docker layers
  - Push images to GitHub Container Registry (optional)
- [ ] Integrate Docker-based test orchestrator
  - Run docker-compose or individual containers
  - Mount test-data volume
  - Collect results from containers
  - Validate against reference
  - Fail on tolerance violations
- [ ] Add performance regression detection
  - Track container execution times
  - Monitor Docker resource usage (cpu, memory)
  - Alert on significant slowdowns
  - Store historical performance data
- [ ] Configure result artifacts
  - Upload test reports
  - Publish benchmark results
  - Upload generated charts and visualizations
  - Save Docker build logs
- [ ] Set up result collection pipeline
  - Store CI run results in persistent storage (GitHub Pages, S3, or database)
  - Generate trend graphs comparing current vs. historical results
  - Publish interactive dashboard with latest results
- [ ] Add status badges to README
- [ ] Document CI/CD Docker usage
  - How to reproduce CI builds locally
  - Docker image management
  - Troubleshooting container issues

**Deliverables**:
- `/.github/workflows/test.yml` - Main CI workflow using Docker
- `/.github/workflows/benchmark.yml` - Performance tracking
- `/.github/workflows/build-images.yml` - Docker image building and caching
- CI/CD documentation with Docker instructions

---

## Phase 6: Expansion & Enhancement
**Goal**: Add more test cases and advanced features

### Tasks
- [ ] Expand test case library
  - More orbital regimes
  - Different observer locations
  - Multi-day visibility windows
  - Doppler shift calculations (optional)
  - Light pollution effects (optional)
- [ ] Enhance visualization tools
  - Ground track plotting (satellite path over Earth)
  - Sky chart generation (azimuth/elevation polar plots)
  - Visibility timeline graphs (elevation vs. time)
  - Error distribution charts (histogram of deltas from reference)
  - Language comparison matrix (heatmap of accuracy/performance)
- [ ] Create web dashboard
  - Live test results from latest CI run
  - Historical benchmarks with trend lines
  - Implementation comparison tables and charts
  - Interactive filtering by test case, language, date range
  - Drill-down into specific test failures or outliers
- [ ] Add stress testing
  - Large satellite constellations
  - Extended time periods
  - High-frequency sampling

**Deliverables**:
- Expanded test case library
- Comprehensive visualization toolkit
- Interactive web dashboard
- Stress test suite

---

## Phase 7: Documentation & Community
**Goal**: Comprehensive documentation and contribution guidelines

### Tasks
- [ ] Write comprehensive README
  - Project overview
  - Quick start guide
  - Architecture explanation
- [ ] Create contribution guidelines
  - How to add a new language
  - How to add test cases
  - Code review process
- [ ] Document all APIs and interfaces
  - Test data format specification
  - Implementation interface contract
  - Validation engine API
- [ ] Add examples and tutorials
  - Running tests locally
  - Interpreting results
  - Debugging failures
- [ ] Create comparison report
  - Library maturity across languages
  - Performance characteristics
  - Accuracy comparison

**Deliverables**:
- Complete documentation suite
- `CONTRIBUTING.md`
- Example tutorials

---

## Success Metrics

### Correctness
- [ ] All implementations agree within tolerance (99%+ test cases)
- [ ] Zero false positives in visibility detection
- [ ] Accurate timing to Â±1 second

### Performance
- [ ] Fastest implementation baseline established
- [ ] No implementation >10x slower than baseline
- [ ] CI pipeline completes in <5 minutes

### Maintainability
- [ ] New language can be added in <4 hours
- [ ] New test case can be added in <30 minutes
- [ ] CI failures provide clear actionable feedback

### Coverage
- [ ] At least 4 languages implemented
- [ ] At least 2-3 libraries tested per language (8-12 total implementations)
- [ ] At least 20 diverse test cases
- [ ] 90%+ code coverage in test runner
- [ ] Cross-library comparison report for each language

### Library Validation
- [ ] Identify which libraries consistently agree with reference
- [ ] Document any library-specific bugs or deviations
- [ ] Provide clear recommendations for library selection per language

---

## Risk Mitigation

### Technical Risks
- **Library availability**: Not all languages have mature SGP4 libraries
  - Mitigation: Start with proven ecosystems, document library gaps
- **Floating-point precision**: Different platforms may yield slightly different results
  - Mitigation: Generous tolerances, platform-specific expectations
- **CI resource limits**: GitHub Actions has time/resource constraints
  - Mitigation: Parallel execution, caching, test case prioritization

### Operational Risks
- **Test data staleness**: TLE data becomes outdated
  - Mitigation: Use synthetic/historical data, document test epoch
- **Breaking changes**: Library updates may change behavior
  - Mitigation: Pin dependencies, version testing

---

## Next Steps

1. Review and approve this plan
2. Set up Git repository and initial structure
3. Begin Phase 1: Define test data schema
4. Identify and document reference libraries for each target language
